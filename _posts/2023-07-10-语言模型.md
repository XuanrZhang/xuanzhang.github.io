---
layout:     post
title:      Generative AI with Large Language Models
subtitle:   Coursera - Deeplearning.AI
date:       2023-07-10
author:     Xuan
header-img: img/post-bg-git.jpg
catalog: true
tags:
    - Coursera -LLM
---


# Week 1 - Introduction to LLMs and the generative AI project life cycle

## Basic definition

- prompt: the input of the model -> the test that you pass into the model
- context window: the space/memory that available to the prompt
- completion: the output of the model
- inference: the act of using the model to generate answers

### Text generation before transformer

> It's important to note that generative algorithms are not new. Previous generations of language models made use of an architecture called recurrent neural networks or RNNs

the limitation of RNN : amount of computation and memory.

An example of RNN carrying out a **simple next-word prediction generative task**

### Why the transformer architecture are powerful?

> The power of the transformer architecture lies in its ability to learn the relevance and context of all of the words in a sentence. Not just as you see here, to each word next to its neighbor, but to every other word in a sentence. 

> Attention weights are apply to those relationships

> Attention Map

> Self-attention and the ability to learn a tension in this way across the whole input significantly approves the model's ability to encode language.

### Steps  

1. Tokenize the words

> machine-learning models are just big statistical calculators and they work with numbers, not words. So before passing texts into the model to process, you must first tokenize the words. 

