---
layout:     post
title:      Generative AI with Large Language Models
subtitle:   Coursera - Deeplearning.AI
date:       2023-07-10
author:     Xuan
header-img: img/post-bg-1.jpg
catalog: true
tags:
    - Coursera -LLM
---


# Week 1 - Introduction to LLMs and the generative AI project life cycle

## Basic definition

- prompt: the input of the model -> the test that you pass into the model
- context window: the space/memory that available to the prompt
- completion: the output of the model
- inference: the act of using the model to generate answers

Base models
![Base models](/img/post-ct-LLMtype.png)

### Text generation before transformer

> It's important to note that generative algorithms are not new. Previous generations of language models made use of an architecture called recurrent neural networks or RNNs

the limitation of RNN : amount of computation and memory.

An example of RNN carrying out a **simple next-word prediction generative task**

### Why the transformer architecture are powerful?

> The power of the transformer architecture lies in its ability to learn the relevance and context of all of the words in a sentence. Not just as you see here, to each word next to its neighbor, but to every other word in a sentence. 

> Attention weights are apply to those relationships

> Attention Map

Self-attention architecture
> Self-attention and the ability to learn a tension in this way across the whole input significantly approves the model's ability to encode language.

![Self-attention](/img/post-ct-LLM-selfattention.png)


### Self-attention Steps  

1. Tokenize the words

> machine-learning models are just big statistical calculators and they work with numbers, not words. So before passing texts into the model to process, you must first tokenize the words.

![Tokenizer](/img/post-ct-LLM-tokenizer.png)

2. numbers passed to the embedding layer

> This layer is a trainable vector embedding space, a high-dimensional space where each token is represented as a vector and occupies a unique location within that space. Each token ID in the vocabulary is matched to a multi-dimensional vector, and the intuition is that these vectors learn to encode the meaning and context of individual tokens in the input sequence. 
> Embedding vector spaces have been used in natural language processing for some time, previous generation language algorithms like Word2vec use this concept.
