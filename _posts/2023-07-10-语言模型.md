---
layout:     post
title:      Generative AI with Large Language Models
subtitle:   Coursera - Deeplearning.AI
date:       2023-07-10
author:     Xuan
header-img: img/post-bg-1.jpg
catalog: true
tags:
    - Coursera -LLM
---


# Week 1 - Introduction to LLMs and the generative AI project life cycle

## Basic definition

- prompt: the input of the model -> the test that you pass into the model
- context window: the space/memory that available to the prompt
- completion: the output of the model
- inference: the act of using the model to generate answers

Base models
![Base models](/img/post-ct-LLMtype.png)

### Text generation before transformer

> It's important to note that generative algorithms are not new. Previous generations of language models made use of an architecture called recurrent neural networks or RNNs

the limitation of RNN : amount of computation and memory.

An example of RNN carrying out a **simple next-word prediction generative task**

### Why the transformer architecture are powerful?

> The power of the transformer architecture lies in its ability to learn the relevance and context of all of the words in a sentence. Not just as you see here, to each word next to its neighbor, but to every other word in a sentence. 

> Attention weights are apply to those relationships

> Attention Map

Self-attention architecture
> Self-attention and the ability to learn a tension in this way across the whole input significantly approves the model's ability to encode language.

![Self-attention](/img/post-LLM-model.png)

> The encoder encodes input sequences into a deep representation of the structure and meaning of the input. 
> The decoder, working from input token triggers, uses the encoder's contextual understanding to generate new tokens. It does this in a loop until some stop condition has been reached.


### Self-attention Steps  

1. Tokenize the words

> machine-learning models are just big statistical calculators and they work with numbers, not words. So before passing texts into the model to process, you must first tokenize the words.

![Tokenizer](/img/post-ct-LLM-tokenizer.png)

2. numbers passed to the embedding layer

> This layer is a trainable vector embedding space, a high-dimensional space where each token is represented as a vector and occupies a unique location within that space. Each token ID in the vocabulary is matched to a multi-dimensional vector, and the intuition is that these vectors learn to encode the meaning and context of individual tokens in the input sequence. 
> Embedding vector spaces have been used in natural language processing for some time, previous generation language algorithms like Word2vec use this concept.

### Variants on the basic self-attention model

![three-types](/img/post-LLM-variants.png)

- Encoder-only models (BERT / T5)

> also work as sequence-to-sequence models, but without further modification, the input sequence and the output sequence or the same length. Their use is less common these days, but by adding additional layers to the architecture, you can train encoder-only models to perform classification tasks such as sentiment analysis.
> BERT is an example of an encoder-only model. 

- Encoder-decoder models (BART)

> as you've seen, perform well on sequence-to-sequence tasks such as translation, where the input sequence and the output sequence can be different lengths. You can also scale and train this type of model to perform general text generation tasks. Examples of encoder-decoder models include BART as opposed to BERT and T5, the model that you'll use in the labs in this course.

- Decoder-only models (GPT family)

> some of the most commonly used today. These models can now generalize to most tasks. Popular decoder-only models include the GPT family of models, BLOOM, Jurassic, LLaMA, and many more.

### Original paper summary - "Attention is All You Need"

> The Transformer architecture consists of an encoder and a decoder, each of which is composed of several layers. 
> Each layer consists of two sub-layers: a multi-head self-attention mechanism and a feed-forward neural network.

- The multi-head self-attention mechanism allows the model to attend to different parts of the input sequence.
- The feed-forward network applies a point-wise fully connected layer to each position separately and identically. 

![paper](/img/post-LLM-attention.png)

### Prompt engineering

- in-context learning : give examples in prompt 

    - zero-shot inference
    ![zero-shot](/img/post-LLM-zeroshot.png)
    > This method, including your input data within the prompt, is called zero-shot inference. The largest of the LLMs are surprisingly good at this, grasping the task to be completed and returning a good answer.

    - one-shot inference
    ![one-shot](/img/post-LLM-oneshot.png)

