---
layout:     post
title:      CNN学习笔记-Learning Notes
subtitle:   Network architecture & Function & How dose it work？
date:       2021-08-25
author:     Xuan
header-img: img/home-bg-AI2.png
catalog: true
tags:
    - CNN
    - Convention
---



#### **CNN**：

实际上是一个不断提取特征 --> 进行特征选择 --> 进行分类的过程

CNN主要由3钟模块构成：

- 卷积层, 提取特征

- 采样层, 特征选择

- 全连接层, 进行分类

卷积神经网络的出现，以参数少，训练快，得分高，易迁移的特点全面碾压之前的简单神经网络


###### 卷积层：输出特征图（feature map）

**输入的只是原始图片，我们还没有提取图片的特征， 我们目前要处理的参数仍然非常多，我们需要对原始输入进行降维或者减少参数**

![简单的网络](/img/post-CNN-1.png)

##### 采样层 （Pooling/subsample）：

实际上就是**一个特征选择的过程**：主要提取边缘特征，对于灰色和黑色这种冗余或者不重要的的区域特征，我们尽量丢弃或者少保留，那么这样可能会减少参数或者减少提参数的过程**

![简单的网络](/img/post-CNN-2.png)

#### 级联分类器（cascade of classifiers）：

大概意思就是我从一堆弱分类器里面，挑出一个最符合要求的弱分类器，用着这个弱分类器把不想要的数据剔除，保留想要的数据

然后再从剩下的弱分类器里，再挑出一个最符合要求的弱分类器，对上一级保留的数据，把不想要的数据剔除，保留想要的数据。

**通过不断串联几个弱分类器，进过数据层层筛选，最后得到我们想要的数据**

![简单的网络](/img/post-CNN-3.png)


- 通过第一个卷积层提取最初特征，输出特征图（feature map）
- 通过第一个采样层对最初的特征图（feature map ）进行特征选择，去除多余特征,重构新的特征图
- 第二个卷积层是对上一层的采样层的输出特征图（feature map）进行二次特征提取
- 第二个采样层也对上层输出进行二次特征选择
- 全连接层就是根据得到的特征进行分类



#### 卷积层 具体做了什么，以及怎么做的：


**卷积的概念** 系统某一时刻的输出是由多个输入共同作用（叠加）的结果。

![简单的网络](/img/post-CNN-4.png)

- 卷积核放在神经网络里，就代表对应的<u>权重（weight)</u>**

- 卷积核和图像进行点乘（dot product),** **就代表卷积核里的权重单独对相应位置的Pixel进行作用**

- 卷积输出的特征图（feature map）,除了特征值本身外，还包含相对位置信息


**实现卷积运算**（先从左到右，再从上到下，直到所有pixels都被卷积核过了一遍）

- 就是从左到右，每隔x列Pixel，向右移动一次卷积核进行卷积(x可以自己定义)
- 从上到下，每隔X行pixel,向下移动一次卷积核，移动完成，再继续如上所述，从左到右进行

x我们叫作**stride,就是步长**的意思，如果我们x = 2, 就是相当每隔两行或者两列进行卷积


![简单的网络](/img/post-CNN-5.png)


**补0（zero padding）**： pixel 外面还围了一圈0.

添了一圈0，实际上什么信息也没有添，但是,stride x=1 的情况下，补0比原来没有添0 的情况下进行卷积，从左到右，从上到下都多赚了2次卷积

<u>好处</u>

- 获得的更多更细致的特征信息，e.g.获得**更多的图像边缘信息**

- 控制卷积层输出的特征图的size，从而可以达到**控制网络结构的作用**

<img src="/img/post-CNN-6.png" alt="zero-pooling" style="zoom:85%;" />


#### **采样层 具体做了什么，以及怎么做的：**

**目的**： 进行<u>特征选择</u>，信息过滤的过程。会损失一小部分信息，降低了计算量

目前有两种主流的采样方法：

- max-pooling，能更好的克服邻域大小受限问题，更多的保留图像的背景信息
- average -pooling，能更好的克服卷积层权值参数误差，更多的保留纹理信息

采样方法的具体说明：

- **max pooling** ：在一个区域内选出最能代表特征的值，e.g.最大值

![简单的网络](/img/post-CNN-7.png)


因此，上图的**池化**也可以理解为卷积核<u>每空两格做一次卷积（stripe=2）</u>，<u>卷积核的大小是2x2</u>， 但是卷积核的作用是**取区域最大值**（即特征最明显的值），而不是做卷积运算

- **average pooling**：把一个区域里的所以值求一个平均值

**激活函数（Activation Function） 具体做了什么，以及怎么做的：**

实际上，采样层出来的结果，会先进入到一个激活函数(activation function), 激活函数的输出值，可以直接用于分类

**激活函数的种类**非常多，选择激活函数时，都要考虑输入输出以及数据变化。

- 最简单的二分类非线性激活函数开始---阶跃函数（Step Function）
- sigmoid ：只会输出正数，以及靠近0的输出变化率最大
- tanh： tanh和sigmoid不同的是，tanh输出可以是负数
- ReLu：输入只能大于0, 如果你输入含有负数，ReLu就不适合，如果你的输入是图片格式，ReLu就挺常用的

![简单的网络](/img/post-CNN-8.png)

#### 全连接层（Fully Connected Layer）具体做了什么 怎么做的：

#### [CNN 入门讲解：什么是全连接层（Fully Connected Layer）?](https://zhuanlan.zhihu.com/p/33841176)




Reference:

[卷积神经网络（CNN）入门讲解-专栏-蒋竺波]https://www.zhihu.com/column/c_141391545

